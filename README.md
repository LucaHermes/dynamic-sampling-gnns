# Graph Learning by Dynamic Sampling
Graph neural networks based on message-passing rely on the principle of neighborhood aggregation which has shown to work well for many graph tasks. In other cases these approaches appear insufficient, for example, when graphs are heterophilic. In such cases, it can help to modulate the aggregation method depending on the characteristic of the current neighborhood. Furthermore, when considering higher-order relations, heterophilic settings become even more important.
In this work, we investigate a sparse version of message-passing that allows selective neighbor integration and aims for learning to identify most salient nodes that are then integrated over. In our approach, information on individual nodes is encoded by generating distinct walks. Because these walks follow distinct trajectories, the higher-order neighborhood grows only linearly which mitigates information bottlenecks. Overall, we aim to find the most salient substructures by deploying a learnable sampling strategy. We validate our method on commonly used graph benchmarks and show the effectiveness especially in heterophilic graphs. We finally discuss possible extensions to the framework.

![dynamic_sampling_main_figure](https://github.com/LucaHermes/dynamic-sampling-gnns/assets/30961397/a95b7fa5-77f8-4ef7-8e8e-7c1dc01519e8)
<sub><br><b>Figure:</b> Overview of the dynamic sampling GNN framework. Left: An individual walker (dark blue) sampling two steps, updating its own state (solid blue arrows) and the node features of the origin node (solid black arrows). Right: Steps performed to sample a single step -- here from node 1 to node 2: Model $p_\theta$ computes edge logits (1) from which the trajectory is sampled to select a single neighbor (2). The walker traverses to the selected neighbor and updates its own state using $s_\phi$ (3). Finally, the walker updates its origin node (4). If multiple walkers (here red and green)---that originated in the same node 1---meet, their states contribute equally to the node update (4). Computations are denoted by solid black arrows, yellow boxes denote parameterized functions and dashed lines denote sampling trajectories.</sub>
